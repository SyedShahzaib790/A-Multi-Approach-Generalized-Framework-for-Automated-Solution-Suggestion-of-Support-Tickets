{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install allennlp\n",
    "# !pip install bert_score\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import spacy\n",
    "import re\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cloud_user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLEN_PATH = \"-- Set Respective Path HERE --\" \n",
    "predictor = Predictor.from_path(ALLEN_PATH+\"bert-base-srl-2019.06.17.tar.gz\")\n",
    "\n",
    "# from allennlp.predictors.predictor import Predictor as allen_ner\n",
    "# predictor_ner = allen_ner.from_path(PATH+\"ner-model-2018.12.18.tar.gz\")\n",
    "\n",
    "# from allennlp.predictors.predictor import Predictor as Predictor_dep\n",
    "# predictor_dep = Predictor_dep.from_path(PATH+\"biaffine-dependency-parser-ptb-2018.08.23.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"-- Set Respective Path HERE --\" \n",
    "final_to_consider = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Action Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_verb_noun(text):\n",
    "    \n",
    "    try:\n",
    "        df = get_spacy_tokens(text)\n",
    "        all_verbs = list(set(df[df.pos == 'VERB']['text'].values.tolist()))\n",
    "        all_nouns = list(set(df[df.pos == 'NOUN']['text'].values.tolist()))\n",
    "\n",
    "        verb_noun = []\n",
    "        for n in all_nouns:\n",
    "            n_verb = df.loc[[n]]['parent'].values.tolist()\n",
    "            common = set(n_verb) & set(all_verbs)\n",
    "            if len(common) > 0:\n",
    "                verb_noun.append((list(common)[0], n))\n",
    "                \n",
    "        if len(verb_noun) < 1:\n",
    "            if (len(all_verbs)>0) and (len(all_nouns)>0) :\n",
    "                verb_noun.append((all_verbs[0], all_nouns[0]))\n",
    "            \n",
    "        return [' '.join(list(a)) for a in verb_noun]\n",
    "    except:\n",
    "        print (text)\n",
    "        raise Exception\n",
    "\n",
    "def get_spacy_tokens(sent):\n",
    "    struct = []\n",
    "    for token in nlp(sent):\n",
    "        struct.append([token.text, token.tag_ , token.pos_, token.dep_, token.head.text, token.head.pos_])\n",
    "    df = pd.DataFrame(struct, columns=['text','tag','pos','dep','parent', 'parent_pos'])\n",
    "    df.index = df.text.values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEMANTIC ROLE LABELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allennlp_action_verbs(sent):\n",
    "    global predictor\n",
    "    \n",
    "    actions = predictor.predict(sentence=sent)\n",
    "    if len(actions['verbs']) < 1:\n",
    "        actions = predictor.predict(sentence=sent.lower())\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parsed_allen_results(txt):\n",
    "    \n",
    "    txt_lower = txt.lower()\n",
    "    result = get_allennlp_action_verbs(txt)\n",
    "    \n",
    "    if txt != txt_lower:\n",
    "        result_2 = get_allennlp_action_verbs(txt_lower)\n",
    "        result['verbs'].extend(result_2['verbs'])\n",
    "\n",
    "    verb_descriptions = []\n",
    "    verbs = []\n",
    "    for verb in result['verbs']:\n",
    "        if (verb['verb'] in verbs) or (verb['verb'].lower() in verbs):\n",
    "            continue\n",
    "        verb_descriptions.append(verb['description'])\n",
    "        verbs.append(verb['verb'].lower())\n",
    "        \n",
    "    return verb_descriptions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARSING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_verb_obj_phrases(allen_sents):\n",
    "    action_obj = []\n",
    "    for sent in allen_sents:\n",
    "        verb = re.findall('\\[V:(.*?)\\]',sent)\n",
    "        arg1 = re.findall('\\[ARG1:(.*?)\\]',sent)\n",
    "        if (len(arg1) > 0) and (len(verb) > 0):\n",
    "            try:\n",
    "                action_obj.append((verb[0].strip() + \" \" + arg1[0].strip()).lower())\n",
    "            except:\n",
    "                print (sent)\n",
    "                raise Exception\n",
    "    return action_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_invalid_action_phrases(action_list):\n",
    "    global stopWords\n",
    "    \n",
    "    new_list = []\n",
    "    for action in action_list:\n",
    "        dep_tree = get_spacy_tokens(action)\n",
    "        tokens = dep_tree.index\n",
    "        tags = dep_tree.tag.values\n",
    "        if tags.shape[0] < 1:\n",
    "            continue\n",
    "        if (tags[0] in ['VBN','VBD']) and (tokens[0] not in stopWords) and (('NN' in tags) or ('NNS' in tags)):\n",
    "            new_list.append(action)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUSTOM PIPELINE FOR ACTION PHRASE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(_input, pipeline_):\n",
    "    if len(pipeline_) == 0:\n",
    "        return _input\n",
    "    \n",
    "    func = pipeline_[0]\n",
    "    output = func(_input) \n",
    "    return run_pipeline(output, pipeline_[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Duplicate Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_subdups_phrases(l_):\n",
    "    for action in copy.deepcopy(l_):\n",
    "        l_temp = copy.deepcopy(l_)\n",
    "        l_temp.remove(action)\n",
    "#         print (action)\n",
    "        if len(re.findall(re.escape(action), ','.join(l_temp))) > 0:\n",
    "            l_.remove(action)\n",
    "    return l_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [get_parsed_allen_results, parse_verb_obj_phrases, filter_invalid_action_phrases, remove_subdups_phrases]\n",
    "# pipeline = [filter_invalid_action_phrases, remove_subdups_phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_to_consider.to_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_to_consider['res_actions'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error = []\n",
    "for idx, val in final_to_consider.iterrows():\n",
    "    try:\n",
    "        if val['res_actions'] != '':\n",
    "            continue\n",
    "#         print(val['res_actions'])\n",
    "        final_to_consider.loc[idx,'res_actions'] = str(run_pipeline(copy.deepcopy(val['ticket_obj'].resolution.text), copy.deepcopy(pipeline)))\n",
    "    except Exception as e:\n",
    "        print ('Error at : ',idx, e)\n",
    "#         raise Exception\n",
    "        error.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_to_consider.loc[:,'res_actions'] = final_to_consider.ticket_obj.map(lambda x: run_pipeline(x.resolution.text, pipeline))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_to_consider = final_to_consider[final_to_consider.res_actions.map(len)!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUMP DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_to_consider.to_pickle(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ticket_resolution]",
   "language": "python",
   "name": "conda-env-ticket_resolution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
